{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Baiyes Classifier\n",
    "\n",
    "- Based on Bayes theorem for calculating probability\n",
    "- P(A|B)=(P(B|A)*P(A))/P(B)\n",
    "    - A-->output\n",
    "    - B--->feature data\n",
    "- guassian Nb\n",
    "- Multinomial Nb(in your target you have only more than 2 labels)\n",
    "- bernouli Nb(in your target you have only 2 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply gaussian Nb to wine dataset and compare the accuracy\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine=load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "         1.065e+03],\n",
       "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "         1.050e+03],\n",
       "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "         1.185e+03],\n",
       "        ...,\n",
       "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "         8.350e+02],\n",
       "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "         8.400e+02],\n",
       "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "         5.600e+02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'),\n",
       " 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n',\n",
       " 'feature_names': ['alcohol',\n",
       "  'malic_acid',\n",
       "  'ash',\n",
       "  'alcalinity_of_ash',\n",
       "  'magnesium',\n",
       "  'total_phenols',\n",
       "  'flavanoids',\n",
       "  'nonflavanoid_phenols',\n",
       "  'proanthocyanins',\n",
       "  'color_intensity',\n",
       "  'hue',\n",
       "  'od280/od315_of_diluted_wines',\n",
       "  'proline']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GaussianNB()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.naive_bayes in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.naive_bayes\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These\n",
      "    are supervised learning methods based on applying Bayes' theorem with strong\n",
      "    (naive) feature independence assumptions.\n",
      "\n",
      "CLASSES\n",
      "    _BaseDiscreteNB(_BaseNB)\n",
      "        BernoulliNB\n",
      "        CategoricalNB\n",
      "        ComplementNB\n",
      "        MultinomialNB\n",
      "    _BaseNB(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        GaussianNB\n",
      "    \n",
      "    class BernoulliNB(_BaseDiscreteNB)\n",
      "     |  BernoulliNB(*, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  \n",
      "     |  Like MultinomialNB, this classifier is suitable for discrete data. The\n",
      "     |  difference is that while MultinomialNB works with occurrence counts,\n",
      "     |  BernoulliNB is designed for binary/boolean features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  binarize : float or None, default=0.0\n",
      "     |      Threshold for binarizing (mapping to booleans) of sample features.\n",
      "     |      If None, input is presumed to already consist of binary vectors.\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes)\n",
      "     |      Log probability of each class (smoothed).\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical log probability of features given a class, P(x_i|y).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
      "     |  >>> from sklearn.naive_bayes import BernoulliNB\n",
      "     |  >>> clf = BernoulliNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  BernoulliNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
      "     |  \n",
      "     |  A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
      "     |  Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
      "     |  Text Categorization, pp. 41-48.\n",
      "     |  \n",
      "     |  V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
      "     |  naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BernoulliNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class CategoricalNB(_BaseDiscreteNB)\n",
      "     |  CategoricalNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for categorical features\n",
      "     |  \n",
      "     |  The categorical Naive Bayes classifier is suitable for classification with\n",
      "     |  discrete features that are categorically distributed. The categories of\n",
      "     |  each feature are drawn from a categorical distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  category_count_ : list of arrays of shape (n_features,)\n",
      "     |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      "     |      for each feature. Each array provides the number of samples\n",
      "     |      encountered for each class and category of the specific feature.\n",
      "     |  \n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Smoothed empirical log probability for each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  feature_log_prob_ : list of arrays of shape (n_features,)\n",
      "     |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      "     |      for each feature. Each array provides the empirical log probability\n",
      "     |      of categories given the respective feature and class, ``P(x_i|y)``.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import CategoricalNB\n",
      "     |  >>> clf = CategoricalNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  CategoricalNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategoricalNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features. Here, each feature of X is\n",
      "     |          assumed to be from a different categorical distribution.\n",
      "     |          It is further assumed that all categories of each feature are\n",
      "     |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      "     |          total number of categories for the given feature. This can, for\n",
      "     |          instance, be achieved with the help of OrdinalEncoder.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features. Here, each feature of X is\n",
      "     |          assumed to be from a different categorical distribution.\n",
      "     |          It is further assumed that all categories of each feature are\n",
      "     |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      "     |          total number of categories for the given feature. This can, for\n",
      "     |          instance, be achieved with the help of OrdinalEncoder.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ComplementNB(_BaseDiscreteNB)\n",
      "     |  ComplementNB(*, alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
      "     |  \n",
      "     |  The Complement Naive Bayes classifier described in Rennie et al. (2003).\n",
      "     |  \n",
      "     |  The Complement Naive Bayes classifier was designed to correct the \"severe\n",
      "     |  assumptions\" made by the standard Multinomial Naive Bayes classifier. It is\n",
      "     |  particularly suited for imbalanced data sets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <complement_naive_bayes>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Only used in edge case with a single class in the training set.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. Not used.\n",
      "     |  \n",
      "     |  norm : bool, default=False\n",
      "     |      Whether or not a second normalization of the weights is performed. The\n",
      "     |      default behavior mirrors the implementations found in Mahout and Weka,\n",
      "     |      which do not follow the full algorithm described in Table 9 of the\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      "     |      Smoothed empirical log probability for each class. Only used in edge\n",
      "     |      case with a single class in the training set.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  feature_all_ : ndarray of shape (n_features,)\n",
      "     |      Number of samples encountered for each feature during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature) during fitting.\n",
      "     |      This value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical weights for class complements.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import ComplementNB\n",
      "     |  >>> clf = ComplementNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ComplementNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
      "     |  Tackling the poor assumptions of naive bayes text classifiers. In ICML\n",
      "     |  (Vol. 3, pp. 616-623).\n",
      "     |  https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ComplementNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, norm=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GaussianNB(_BaseNB)\n",
      "     |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      "     |  \n",
      "     |  Gaussian Naive Bayes (GaussianNB)\n",
      "     |  \n",
      "     |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      "     |  For details on algorithm used to update feature means and variance online,\n",
      "     |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      "     |  \n",
      "     |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  priors : array-like of shape (n_classes,)\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  var_smoothing : float, default=1e-9\n",
      "     |      Portion of the largest variance of all features that is added to\n",
      "     |      variances for calculation stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      number of training samples observed in each class.\n",
      "     |  \n",
      "     |  class_prior_ : ndarray of shape (n_classes,)\n",
      "     |      probability of each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      class labels known to the classifier\n",
      "     |  \n",
      "     |  epsilon_ : float\n",
      "     |      absolute additive value to variances\n",
      "     |  \n",
      "     |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      "     |      variance of each feature per class\n",
      "     |  \n",
      "     |  theta_ : ndarray of shape (n_classes, n_features)\n",
      "     |      mean of each feature per class\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> clf = GaussianNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  GaussianNB()\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  >>> clf_pf = GaussianNB()\n",
      "     |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      "     |  GaussianNB()\n",
      "     |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Gaussian Naive Bayes according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance and numerical stability overhead,\n",
      "     |      hence it is better to call partial_fit on chunks of data that are\n",
      "     |      as large as possible (as long as fitting in the memory budget) to\n",
      "     |      hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes,), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultinomialNB(_BaseDiscreteNB)\n",
      "     |  MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |  \n",
      "     |  Naive Bayes classifier for multinomial models\n",
      "     |  \n",
      "     |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      "     |  discrete features (e.g., word counts for text classification). The\n",
      "     |  multinomial distribution normally requires integer feature counts. However,\n",
      "     |  in practice, fractional counts such as tf-idf may also work.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : bool, default=True\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like of shape (n_classes,), default=None\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_count_ : ndarray of shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  class_log_prior_ : ndarray of shape (n_classes, )\n",
      "     |      Smoothed empirical log probability for each class.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      "     |      Empirical log probability of features\n",
      "     |      given a class, ``P(x_i|y)``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_classes, )\n",
      "     |      Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      Number of features of each sample.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(1)\n",
      "     |  >>> X = rng.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "     |  >>> clf = MultinomialNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  MultinomialNB()\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      "     |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      "     |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultinomialNB\n",
      "     |      _BaseDiscreteNB\n",
      "     |      _BaseNB\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like of shape (n_classes), default=None\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\india\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('sklearn.naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814814814815"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GaussianNB in module sklearn.naive_bayes object:\n",
      "\n",
      "class GaussianNB(_BaseNB)\n",
      " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      " |  \n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like of shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, default=1e-9\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      class labels known to the classifier\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances\n",
      " |  \n",
      " |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  theta_ : ndarray of shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes,), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### with new dataset\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"https://raw.githubusercontent.com/AP-State-Skill-Development-Corporation/Datasets/master/Classification/Orange_Telecom_Churn_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>...</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>number_customer_service_calls</th>\n",
       "      <th>churned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>371-7191</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>358-1921</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>375-9999</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>330-6626</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>HI</td>\n",
       "      <td>50</td>\n",
       "      <td>408</td>\n",
       "      <td>365-8751</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>40</td>\n",
       "      <td>235.7</td>\n",
       "      <td>127</td>\n",
       "      <td>40.07</td>\n",
       "      <td>...</td>\n",
       "      <td>126</td>\n",
       "      <td>18.96</td>\n",
       "      <td>297.5</td>\n",
       "      <td>116</td>\n",
       "      <td>13.39</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>WV</td>\n",
       "      <td>152</td>\n",
       "      <td>415</td>\n",
       "      <td>334-9736</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>184.2</td>\n",
       "      <td>90</td>\n",
       "      <td>31.31</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>21.83</td>\n",
       "      <td>213.6</td>\n",
       "      <td>113</td>\n",
       "      <td>9.61</td>\n",
       "      <td>14.7</td>\n",
       "      <td>2</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>DC</td>\n",
       "      <td>61</td>\n",
       "      <td>415</td>\n",
       "      <td>333-6861</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>140.6</td>\n",
       "      <td>89</td>\n",
       "      <td>23.90</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>14.69</td>\n",
       "      <td>212.4</td>\n",
       "      <td>97</td>\n",
       "      <td>9.56</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>DC</td>\n",
       "      <td>109</td>\n",
       "      <td>510</td>\n",
       "      <td>394-2206</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67</td>\n",
       "      <td>32.10</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>14.59</td>\n",
       "      <td>224.4</td>\n",
       "      <td>89</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>VT</td>\n",
       "      <td>86</td>\n",
       "      <td>415</td>\n",
       "      <td>373-8058</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>34</td>\n",
       "      <td>129.4</td>\n",
       "      <td>102</td>\n",
       "      <td>22.00</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>22.70</td>\n",
       "      <td>154.8</td>\n",
       "      <td>100</td>\n",
       "      <td>6.97</td>\n",
       "      <td>9.3</td>\n",
       "      <td>16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     state  account_length  area_code phone_number intl_plan voice_mail_plan  \\\n",
       "0       KS             128        415     382-4657        no             yes   \n",
       "1       OH             107        415     371-7191        no             yes   \n",
       "2       NJ             137        415     358-1921        no              no   \n",
       "3       OH              84        408     375-9999       yes              no   \n",
       "4       OK              75        415     330-6626       yes              no   \n",
       "...    ...             ...        ...          ...       ...             ...   \n",
       "4995    HI              50        408     365-8751        no             yes   \n",
       "4996    WV             152        415     334-9736        no              no   \n",
       "4997    DC              61        415     333-6861        no              no   \n",
       "4998    DC             109        510     394-2206        no              no   \n",
       "4999    VT              86        415     373-8058        no             yes   \n",
       "\n",
       "      number_vmail_messages  total_day_minutes  total_day_calls  \\\n",
       "0                        25              265.1              110   \n",
       "1                        26              161.6              123   \n",
       "2                         0              243.4              114   \n",
       "3                         0              299.4               71   \n",
       "4                         0              166.7              113   \n",
       "...                     ...                ...              ...   \n",
       "4995                     40              235.7              127   \n",
       "4996                      0              184.2               90   \n",
       "4997                      0              140.6               89   \n",
       "4998                      0              188.8               67   \n",
       "4999                     34              129.4              102   \n",
       "\n",
       "      total_day_charge  ...  total_eve_calls  total_eve_charge  \\\n",
       "0                45.07  ...               99             16.78   \n",
       "1                27.47  ...              103             16.62   \n",
       "2                41.38  ...              110             10.30   \n",
       "3                50.90  ...               88              5.26   \n",
       "4                28.34  ...              122             12.61   \n",
       "...                ...  ...              ...               ...   \n",
       "4995             40.07  ...              126             18.96   \n",
       "4996             31.31  ...               73             21.83   \n",
       "4997             23.90  ...              128             14.69   \n",
       "4998             32.10  ...               92             14.59   \n",
       "4999             22.00  ...              104             22.70   \n",
       "\n",
       "      total_night_minutes  total_night_calls  total_night_charge  \\\n",
       "0                   244.7                 91               11.01   \n",
       "1                   254.4                103               11.45   \n",
       "2                   162.6                104                7.32   \n",
       "3                   196.9                 89                8.86   \n",
       "4                   186.9                121                8.41   \n",
       "...                   ...                ...                 ...   \n",
       "4995                297.5                116               13.39   \n",
       "4996                213.6                113                9.61   \n",
       "4997                212.4                 97                9.56   \n",
       "4998                224.4                 89               10.10   \n",
       "4999                154.8                100                6.97   \n",
       "\n",
       "      total_intl_minutes  total_intl_calls  total_intl_charge  \\\n",
       "0                   10.0                 3               2.70   \n",
       "1                   13.7                 3               3.70   \n",
       "2                   12.2                 5               3.29   \n",
       "3                    6.6                 7               1.78   \n",
       "4                   10.1                 3               2.73   \n",
       "...                  ...               ...                ...   \n",
       "4995                 9.9                 5               2.67   \n",
       "4996                14.7                 2               3.97   \n",
       "4997                13.6                 4               3.67   \n",
       "4998                 8.5                 6               2.30   \n",
       "4999                 9.3                16               2.51   \n",
       "\n",
       "      number_customer_service_calls  churned  \n",
       "0                                 1    False  \n",
       "1                                 1    False  \n",
       "2                                 0    False  \n",
       "3                                 2    False  \n",
       "4                                 3    False  \n",
       "...                             ...      ...  \n",
       "4995                              2    False  \n",
       "4996                              3     True  \n",
       "4997                              1    False  \n",
       "4998                              0    False  \n",
       "4999                              0    False  \n",
       "\n",
       "[5000 rows x 21 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop('churned',axis=1)\n",
    "x=data.drop('phone_number',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>total_eve_minutes</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>number_customer_service_calls</th>\n",
       "      <th>churned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>HI</td>\n",
       "      <td>50</td>\n",
       "      <td>408</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>40</td>\n",
       "      <td>235.7</td>\n",
       "      <td>127</td>\n",
       "      <td>40.07</td>\n",
       "      <td>223.0</td>\n",
       "      <td>126</td>\n",
       "      <td>18.96</td>\n",
       "      <td>297.5</td>\n",
       "      <td>116</td>\n",
       "      <td>13.39</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>WV</td>\n",
       "      <td>152</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>184.2</td>\n",
       "      <td>90</td>\n",
       "      <td>31.31</td>\n",
       "      <td>256.8</td>\n",
       "      <td>73</td>\n",
       "      <td>21.83</td>\n",
       "      <td>213.6</td>\n",
       "      <td>113</td>\n",
       "      <td>9.61</td>\n",
       "      <td>14.7</td>\n",
       "      <td>2</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>DC</td>\n",
       "      <td>61</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>140.6</td>\n",
       "      <td>89</td>\n",
       "      <td>23.90</td>\n",
       "      <td>172.8</td>\n",
       "      <td>128</td>\n",
       "      <td>14.69</td>\n",
       "      <td>212.4</td>\n",
       "      <td>97</td>\n",
       "      <td>9.56</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>DC</td>\n",
       "      <td>109</td>\n",
       "      <td>510</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>188.8</td>\n",
       "      <td>67</td>\n",
       "      <td>32.10</td>\n",
       "      <td>171.7</td>\n",
       "      <td>92</td>\n",
       "      <td>14.59</td>\n",
       "      <td>224.4</td>\n",
       "      <td>89</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>VT</td>\n",
       "      <td>86</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>34</td>\n",
       "      <td>129.4</td>\n",
       "      <td>102</td>\n",
       "      <td>22.00</td>\n",
       "      <td>267.1</td>\n",
       "      <td>104</td>\n",
       "      <td>22.70</td>\n",
       "      <td>154.8</td>\n",
       "      <td>100</td>\n",
       "      <td>6.97</td>\n",
       "      <td>9.3</td>\n",
       "      <td>16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     state  account_length  area_code intl_plan voice_mail_plan  \\\n",
       "0       KS             128        415        no             yes   \n",
       "1       OH             107        415        no             yes   \n",
       "2       NJ             137        415        no              no   \n",
       "3       OH              84        408       yes              no   \n",
       "4       OK              75        415       yes              no   \n",
       "...    ...             ...        ...       ...             ...   \n",
       "4995    HI              50        408        no             yes   \n",
       "4996    WV             152        415        no              no   \n",
       "4997    DC              61        415        no              no   \n",
       "4998    DC             109        510        no              no   \n",
       "4999    VT              86        415        no             yes   \n",
       "\n",
       "      number_vmail_messages  total_day_minutes  total_day_calls  \\\n",
       "0                        25              265.1              110   \n",
       "1                        26              161.6              123   \n",
       "2                         0              243.4              114   \n",
       "3                         0              299.4               71   \n",
       "4                         0              166.7              113   \n",
       "...                     ...                ...              ...   \n",
       "4995                     40              235.7              127   \n",
       "4996                      0              184.2               90   \n",
       "4997                      0              140.6               89   \n",
       "4998                      0              188.8               67   \n",
       "4999                     34              129.4              102   \n",
       "\n",
       "      total_day_charge  total_eve_minutes  total_eve_calls  total_eve_charge  \\\n",
       "0                45.07              197.4               99             16.78   \n",
       "1                27.47              195.5              103             16.62   \n",
       "2                41.38              121.2              110             10.30   \n",
       "3                50.90               61.9               88              5.26   \n",
       "4                28.34              148.3              122             12.61   \n",
       "...                ...                ...              ...               ...   \n",
       "4995             40.07              223.0              126             18.96   \n",
       "4996             31.31              256.8               73             21.83   \n",
       "4997             23.90              172.8              128             14.69   \n",
       "4998             32.10              171.7               92             14.59   \n",
       "4999             22.00              267.1              104             22.70   \n",
       "\n",
       "      total_night_minutes  total_night_calls  total_night_charge  \\\n",
       "0                   244.7                 91               11.01   \n",
       "1                   254.4                103               11.45   \n",
       "2                   162.6                104                7.32   \n",
       "3                   196.9                 89                8.86   \n",
       "4                   186.9                121                8.41   \n",
       "...                   ...                ...                 ...   \n",
       "4995                297.5                116               13.39   \n",
       "4996                213.6                113                9.61   \n",
       "4997                212.4                 97                9.56   \n",
       "4998                224.4                 89               10.10   \n",
       "4999                154.8                100                6.97   \n",
       "\n",
       "      total_intl_minutes  total_intl_calls  total_intl_charge  \\\n",
       "0                   10.0                 3               2.70   \n",
       "1                   13.7                 3               3.70   \n",
       "2                   12.2                 5               3.29   \n",
       "3                    6.6                 7               1.78   \n",
       "4                   10.1                 3               2.73   \n",
       "...                  ...               ...                ...   \n",
       "4995                 9.9                 5               2.67   \n",
       "4996                14.7                 2               3.97   \n",
       "4997                13.6                 4               3.67   \n",
       "4998                 8.5                 6               2.30   \n",
       "4999                 9.3                16               2.51   \n",
       "\n",
       "      number_customer_service_calls  churned  \n",
       "0                                 1        0  \n",
       "1                                 1        0  \n",
       "2                                 0        0  \n",
       "3                                 2        0  \n",
       "4                                 3        0  \n",
       "...                             ...      ...  \n",
       "4995                              2        0  \n",
       "4996                              3        1  \n",
       "4997                              1        0  \n",
       "4998                              0        0  \n",
       "4999                              0        0  \n",
       "\n",
       "[5000 rows x 20 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['churned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "4995    0\n",
       "4996    1\n",
       "4997    0\n",
       "4998    0\n",
       "4999    0\n",
       "Name: churned, Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "x['state']=le.fit_transform(x['state'])\n",
    "x['intl_plan']=le.fit_transform(x['intl_plan'])\n",
    "x['voice_mail_plan']=le.fit_transform(x['voice_mail_plan'])\n",
    "data['churned']=le.fit_transform(data['churned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['churned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "4995    0\n",
       "4996    1\n",
       "4997    0\n",
       "4998    0\n",
       "4999    0\n",
       "Name: churned, Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GaussianNB()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logistic regression\n",
    "dataset=pd.read_csv(\"https://raw.githubusercontent.com/AP-State-Skill-Development-Corporation/Datasets/master/Classification/Orange_Telecom_Churn_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                            0\n",
       "account_length                   0\n",
       "area_code                        0\n",
       "phone_number                     0\n",
       "intl_plan                        0\n",
       "voice_mail_plan                  0\n",
       "number_vmail_messages            0\n",
       "total_day_minutes                0\n",
       "total_day_calls                  0\n",
       "total_day_charge                 0\n",
       "total_eve_minutes                0\n",
       "total_eve_calls                  0\n",
       "total_eve_charge                 0\n",
       "total_night_minutes              0\n",
       "total_night_calls                0\n",
       "total_night_charge               0\n",
       "total_intl_minutes               0\n",
       "total_intl_calls                 0\n",
       "total_intl_charge                0\n",
       "number_customer_service_calls    0\n",
       "churned                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\India\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=25)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.73333333333333"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy_score(y_test,y_predict)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##using svm\n",
    "from sklearn.svm import SVC\n",
    "model=SVC()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8606666666666667"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using decision tree and random forest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model2=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model3=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ndata=pd.read_csv(\"amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uncleanedreview</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9312</th>\n",
       "      <td>Good buy and works just like my old one did be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>I ordered one of these a week ago and so far i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>Mostly great...the SIM card kept randomly slid...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>Love this phone. I have had it for about a yea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>I paid one cent for my new Samsung and my cont...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9317 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        uncleanedreview  decision\n",
       "0     I had the Samsung A600 for awhile which is abs...  positive\n",
       "1     Due to a software issue between Nokia and Spri...  positive\n",
       "2     This is a great, reliable phone. I also purcha...  positive\n",
       "3     I love the phone and all, because I really did...  positive\n",
       "4     The phone has been great for every purpose it ...  positive\n",
       "...                                                 ...       ...\n",
       "9312  Good buy and works just like my old one did be...  positive\n",
       "9313  I ordered one of these a week ago and so far i...  negative\n",
       "9314  Mostly great...the SIM card kept randomly slid...  positive\n",
       "9315  Love this phone. I have had it for about a yea...  positive\n",
       "9316  I paid one cent for my new Samsung and my cont...  positive\n",
       "\n",
       "[9317 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    7687\n",
       "negative    1630\n",
       "Name: decision, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndata['decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ndata.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a great, reliable phone. I also purchased this phone after my samsung A460 died. The menu is easily comprehendable and speed dialing is available for around 300 numbers. Voice dialing is also a nice feature, but it takes longer than speed dialing. The only thing that bothers me is the games...Nokia seems to have taken snake (1 and 2) off their phones. There is a skydiving game, bowling, and tennis (like pong). The ringers are very nice, and a feature is available to choose a different ringer for each person calling. However, ringtones are not available online to download to this phone. You're pretty much stuck with what you have. There are vibrating ringtones and regular (midi) polyphonic tones. All they need are covers in a reasonable price range...\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'great', ',', 'reliable', 'phone', '.', 'I', 'also', 'purchased', 'this', 'phone', 'after', 'my', 'samsung', 'A460', 'died', '.', 'The', 'menu', 'is', 'easily', 'comprehendable', 'and', 'speed', 'dialing', 'is', 'available', 'for', 'around', '300', 'numbers', '.', 'Voice', 'dialing', 'is', 'also', 'a', 'nice', 'feature', ',', 'but', 'it', 'takes', 'longer', 'than', 'speed', 'dialing', '.', 'The', 'only', 'thing', 'that', 'bothers', 'me', 'is', 'the', 'games', '...', 'Nokia', 'seems', 'to', 'have', 'taken', 'snake', '(', '1', 'and', '2', ')', 'off', 'their', 'phones', '.', 'There', 'is', 'a', 'skydiving', 'game', ',', 'bowling', ',', 'and', 'tennis', '(', 'like', 'pong', ')', '.', 'The', 'ringers', 'are', 'very', 'nice', ',', 'and', 'a', 'feature', 'is', 'available', 'to', 'choose', 'a', 'different', 'ringer', 'for', 'each', 'person', 'calling', '.', 'However', ',', 'ringtones', 'are', 'not', 'available', 'online', 'to', 'download', 'to', 'this', 'phone', '.', 'You', \"'re\", 'pretty', 'much', 'stuck', 'with', 'what', 'you', 'have', '.', 'There', 'are', 'vibrating', 'ringtones', 'and', 'regular', '(', 'midi', ')', 'polyphonic', 'tones', '.', 'All', 'they', 'need', 'are', 'covers', 'in', 'a', 'reasonable', 'price', 'range', '...']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "data=word_tokenize(a)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"wouldn't\", 'doesn', 'now', 'don', 'its', 'our', 'should', 'couldn', 'his', 'will', 'here', \"haven't\", 'any', 'that', 'up', 'ma', 'all', 'most', 'himself', 'my', 'has', 'then', 'was', 'have', 'when', 'at', 'had', 'because', \"you'd\", 'to', 'wouldn', \"needn't\", 'hadn', 'the', 'nor', 'themselves', \"she's\", \"couldn't\", 'into', 'be', 'weren', 'myself', 'aren', 'who', 'not', 'y', 'been', \"that'll\", 're', 'am', 'ours', 'it', 'shan', \"hasn't\", \"won't\", 'during', 'yourself', 'few', \"aren't\", 'being', 'before', \"hadn't\", 'did', 'herself', 't', 'of', 'which', 'your', 'some', 's', \"wasn't\", 'yourselves', 'once', 'won', 'me', 'itself', \"should've\", 'how', 'o', 'and', 'him', 'out', 'this', 'so', 'ourselves', 'under', \"you've\", 'their', 'her', 'above', \"weren't\", \"you'll\", 'wasn', 'over', 'only', 'very', 'further', 'haven', 'on', 'own', 'through', 'shouldn', 'didn', 'but', 'until', \"you're\", 'with', 'while', \"isn't\", 'down', 'hasn', 'between', 'in', 'd', 'off', 'if', \"doesn't\", 'he', \"don't\", 'yours', 'hers', \"mustn't\", 'an', 'they', 'them', 'having', 'where', \"shouldn't\", 'does', 'from', 'm', 'mightn', \"mightn't\", 've', 'below', 'were', 'for', 'about', 'by', 'against', 'as', 'isn', 'mustn', 'ain', 'whom', 'a', 'what', 'no', 'we', 'doing', 'needn', 'too', 'other', 'can', 'just', 'you', 'such', 'll', 'is', \"shan't\", 'both', 'than', 'these', 'or', 'there', 'are', 'those', 'do', 'same', 'why', 'each', \"it's\", 'more', \"didn't\", 'she', 'theirs', 'i', 'after', 'again'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "b=[i for i in data if i not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'reliable',\n",
       " 'phone',\n",
       " 'I',\n",
       " 'also',\n",
       " 'purchased',\n",
       " 'this',\n",
       " 'phone',\n",
       " 'after',\n",
       " 'my',\n",
       " 'samsung',\n",
       " 'A460',\n",
       " 'died',\n",
       " 'The',\n",
       " 'menu',\n",
       " 'is',\n",
       " 'easily',\n",
       " 'comprehendable',\n",
       " 'and',\n",
       " 'speed',\n",
       " 'dialing',\n",
       " 'is',\n",
       " 'available',\n",
       " 'for',\n",
       " 'around',\n",
       " '300',\n",
       " 'numbers',\n",
       " 'Voice',\n",
       " 'dialing',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'feature',\n",
       " 'but',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'speed',\n",
       " 'dialing',\n",
       " 'The',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'bothers',\n",
       " 'me',\n",
       " 'is',\n",
       " 'the',\n",
       " 'games',\n",
       " '...',\n",
       " 'Nokia',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'snake',\n",
       " '1',\n",
       " 'and',\n",
       " '2',\n",
       " 'off',\n",
       " 'their',\n",
       " 'phones',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'skydiving',\n",
       " 'game',\n",
       " 'bowling',\n",
       " 'and',\n",
       " 'tennis',\n",
       " 'like',\n",
       " 'pong',\n",
       " 'The',\n",
       " 'ringers',\n",
       " 'are',\n",
       " 'very',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'a',\n",
       " 'feature',\n",
       " 'is',\n",
       " 'available',\n",
       " 'to',\n",
       " 'choose',\n",
       " 'a',\n",
       " 'different',\n",
       " 'ringer',\n",
       " 'for',\n",
       " 'each',\n",
       " 'person',\n",
       " 'calling',\n",
       " 'However',\n",
       " 'ringtones',\n",
       " 'are',\n",
       " 'not',\n",
       " 'available',\n",
       " 'online',\n",
       " 'to',\n",
       " 'download',\n",
       " 'to',\n",
       " 'this',\n",
       " 'phone',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'stuck',\n",
       " 'with',\n",
       " 'what',\n",
       " 'you',\n",
       " 'have',\n",
       " 'There',\n",
       " 'are',\n",
       " 'vibrating',\n",
       " 'ringtones',\n",
       " 'and',\n",
       " 'regular',\n",
       " 'midi',\n",
       " 'polyphonic',\n",
       " 'tones',\n",
       " 'All',\n",
       " 'they',\n",
       " 'need',\n",
       " 'are',\n",
       " 'covers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'reasonable',\n",
       " 'price',\n",
       " 'range',\n",
       " '...']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=\"\".join(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ThisisagreatreliablephoneIalsopurchasedthisphoneaftermysamsungA460diedThemenuiseasilycomprehendableandspeeddialingisavailableforaround300numbersVoicedialingisalsoanicefeaturebutittakeslongerthanspeeddialingTheonlythingthatbothersmeisthegames...Nokiaseemstohavetakensnake1and2offtheirphonesThereisaskydivinggamebowlingandtennislikepongTheringersareveryniceandafeatureisavailabletochooseadifferentringerforeachpersoncallingHoweverringtonesarenotavailableonlinetodownloadtothisphoneYou'reprettymuchstuckwithwhatyouhaveTherearevibratingringtonesandregularmidipolyphonictonesAlltheyneedarecoversinareasonablepricerange...\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec=CountVectorizer()\n",
    "vec.fit(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 79,\n",
       " 'is': 30,\n",
       " 'great': 26,\n",
       " 'reliable': 59,\n",
       " 'phone': 48,\n",
       " 'also': 4,\n",
       " 'purchased': 54,\n",
       " 'after': 2,\n",
       " 'my': 38,\n",
       " 'samsung': 63,\n",
       " 'a460': 1,\n",
       " 'died': 17,\n",
       " 'the': 74,\n",
       " 'menu': 35,\n",
       " 'easily': 21,\n",
       " 'comprehendable': 14,\n",
       " 'and': 5,\n",
       " 'speed': 67,\n",
       " 'dialing': 16,\n",
       " 'available': 8,\n",
       " 'for': 23,\n",
       " 'around': 7,\n",
       " '300': 0,\n",
       " 'numbers': 43,\n",
       " 'voice': 84,\n",
       " 'nice': 40,\n",
       " 'feature': 22,\n",
       " 'but': 11,\n",
       " 'it': 31,\n",
       " 'takes': 70,\n",
       " 'longer': 33,\n",
       " 'than': 72,\n",
       " 'only': 46,\n",
       " 'thing': 78,\n",
       " 'that': 73,\n",
       " 'bothers': 9,\n",
       " 'me': 34,\n",
       " 'games': 25,\n",
       " 'nokia': 41,\n",
       " 'seems': 64,\n",
       " 'to': 80,\n",
       " 'have': 27,\n",
       " 'taken': 69,\n",
       " 'snake': 66,\n",
       " 'off': 44,\n",
       " 'their': 75,\n",
       " 'phones': 49,\n",
       " 'there': 76,\n",
       " 'skydiving': 65,\n",
       " 'game': 24,\n",
       " 'bowling': 10,\n",
       " 'tennis': 71,\n",
       " 'like': 32,\n",
       " 'pong': 51,\n",
       " 'ringers': 61,\n",
       " 'are': 6,\n",
       " 'very': 82,\n",
       " 'choose': 13,\n",
       " 'different': 18,\n",
       " 'ringer': 60,\n",
       " 'each': 20,\n",
       " 'person': 47,\n",
       " 'calling': 12,\n",
       " 'however': 28,\n",
       " 'ringtones': 62,\n",
       " 'not': 42,\n",
       " 'online': 45,\n",
       " 'download': 19,\n",
       " 'you': 87,\n",
       " 're': 56,\n",
       " 'pretty': 52,\n",
       " 'much': 37,\n",
       " 'stuck': 68,\n",
       " 'with': 86,\n",
       " 'what': 85,\n",
       " 'vibrating': 83,\n",
       " 'regular': 58,\n",
       " 'midi': 36,\n",
       " 'polyphonic': 50,\n",
       " 'tones': 81,\n",
       " 'all': 3,\n",
       " 'they': 77,\n",
       " 'need': 39,\n",
       " 'covers': 15,\n",
       " 'in': 29,\n",
       " 'reasonable': 57,\n",
       " 'price': 53,\n",
       " 'range': 55}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 79)\t1\n",
      "  (1, 30)\t1\n",
      "  (3, 26)\t1\n",
      "  (4, 59)\t1\n",
      "  (5, 48)\t1\n",
      "  (7, 4)\t1\n",
      "  (8, 54)\t1\n",
      "  (9, 79)\t1\n",
      "  (10, 48)\t1\n",
      "  (11, 2)\t1\n",
      "  (12, 38)\t1\n",
      "  (13, 63)\t1\n",
      "  (14, 1)\t1\n",
      "  (15, 17)\t1\n",
      "  (16, 74)\t1\n",
      "  (17, 35)\t1\n",
      "  (18, 30)\t1\n",
      "  (19, 21)\t1\n",
      "  (20, 14)\t1\n",
      "  (21, 5)\t1\n",
      "  (22, 67)\t1\n",
      "  (23, 16)\t1\n",
      "  (24, 30)\t1\n",
      "  (25, 8)\t1\n",
      "  (26, 23)\t1\n",
      "  :\t:\n",
      "  (108, 52)\t1\n",
      "  (109, 37)\t1\n",
      "  (110, 68)\t1\n",
      "  (111, 86)\t1\n",
      "  (112, 85)\t1\n",
      "  (113, 87)\t1\n",
      "  (114, 27)\t1\n",
      "  (115, 76)\t1\n",
      "  (116, 6)\t1\n",
      "  (117, 83)\t1\n",
      "  (118, 62)\t1\n",
      "  (119, 5)\t1\n",
      "  (120, 58)\t1\n",
      "  (121, 36)\t1\n",
      "  (122, 50)\t1\n",
      "  (123, 81)\t1\n",
      "  (124, 3)\t1\n",
      "  (125, 77)\t1\n",
      "  (126, 39)\t1\n",
      "  (127, 6)\t1\n",
      "  (128, 15)\t1\n",
      "  (129, 29)\t1\n",
      "  (131, 57)\t1\n",
      "  (132, 53)\t1\n",
      "  (133, 55)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer=vec.transform(b)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
